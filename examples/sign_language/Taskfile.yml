version: '3'

dotenv: [.env]

tasks:

  how2sign:train_sentencepiece_lowercased:
    desc: Train SentencePiece models for the How2Sign dataset, for lowercased text.
    preconditions:
      - sh: "[[ -d {{.FAIRSEQ_ROOT}} ]]"
        msg: Specify the FAIRSEQ_ROOT in the .env file.
      - sh: "[ ! -z {{.SAVE_DIR}} ]"
        msg: Specify the SAVE_DIR in the .env file.
      - sh: "[ ! -z {{.VOCAB_SIZE}} ]"
        msg: Specify a VOCAB_SIZE.
    cmds:
      - cmd: |
              PYTHONPATH=$PYTHONPATH:{{.FAIRSEQ_ROOT}} \
              python {{.FAIRSEQ_ROOT}}/examples/sign_language/scripts/train_spm.py \
                --tsv-file {{.SAVE_DIR}}/cvpr23.fairseq.mediapipe.train.how2sign.tsv \
                --spm-prefix {{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_lowercased \
                --vocab-size {{.VOCAB_SIZE}} \
                --vocab-type unigram \
                --lowercase True \
                --column translation
    status:
      - test $(cat {{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_tokenized.vocab | wc -l) -eq {{.VOCAB_SIZE}}
      - test $(cat {{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_tokenized.txt | wc -l) -eq $(({{.VOCAB_SIZE}}-4))
    generates:
      - "{{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_tokenized.model"
      - "{{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_tokenized.txt"
      - "{{.SAVE_DIR}}/vocab/cvpr23.train.how2sign.unigram{{.VOCAB_SIZE}}_tokenized.vocab"
  
  greet:
    cmds:
      - cmd: echo "Testing, {{.DATA_DIR}}!"

  train_slt:
    preconditions:
      - sh: "[[ -d {{.DATA_DIR}} ]]"
        msg: Specify the DATA_DIR in the .env file
      - sh: "[[ {{.SAVE_DIR}} ]]"
        msg: Specify the SAVE_DIR in the .env file
      - sh: "[[ {{.WANDB_ENTITY}} ]]"
        msg: Specify the WANDB_ENTITY in the .env file
      - sh: "[[ {{.WANDB_PROJECT}} ]]"
        msg: Specify the WANDB_PROJECT in the .env file
      - sh: "[ {{.NUM_GPUS}} -gt 0 ]"
        msg: Use at least 1 GPU to train the model
      - sh: '[ "{{.EXPERIMENT}}" ]'
        msg: Specify a name for the EXPERIMENT
      - sh: '[ "{{.CONFIG_DIR}}/{{.EXPERIMENT}}.yaml" ]'
        msg: The yaml does not exist
    vars:
      BASE_UPDATE_FREQ: 1
      NUM_GPUS: 1
      UPDATE_FREQ: 1
    cmds:
      - cmd: echo "Running experiment {{.EXPERIMENT}}"
        silent: true
      - cmd: echo "Running experiment's file {{.CONFIG_DIR}}/{{.EXPERIMENT}}.yaml"
        silent: true

      - cmd: |
              WANDB_NAME={{.EXPERIMENT}} \
              fairseq-hydra-train \
              --config-dir {{.CONFIG_DIR}} \
              --config-name {{.EXPERIMENT}}.yaml \
              optimization.update_freq=[{{.UPDATE_FREQ}}]
        silent: false



  generate:
    preconditions:
      - sh: "[[ -d {{.DATA_DIR}} ]]"
        msg: Specify the DATA_ROOT in the .env file
      - sh: "[[ {{.SAVE_DIR}} ]]"
        msg: Specify the SAVE_DIR in the .env file
      - sh: '[ "{{.EXPERIMENT}}" ]'
        msg: Specify a name for the EXPERIMENT
      - sh: '[ "{{.CKPT}}" ]'
        msg: Specify a CKPT
      - sh: '[ "{{.SUBSET}}" ]'
        msg: Specify a SUBSET
      - sh: '[ "{{.SPM_MODEL}}" ]'
        msg: Specify a SPM_MODEL

    cmds:
      - cmd: echo "Generating outputs for '{{.EXPERIMENT}}' ({{.SUBSET}}) with checkpoint '{{.CKPT}}'"
        silent: true

      - cmd: |
              mkdir -p $(dirname "{{.OUT_FILE}}")
              fairseq-generate {{.I3D_DIR}} \
              --path {{.CKPT_FILE}} \
              --task sign_to_text \
              --gen-subset {{.SUBSET}} \
              --seed {{.SEED}} \
              --batch-size 32 \
              --scoring sacrebleu \
              --max-source-positions {{.MAX_SOURCE_POSITIONS}} \
              --max-target-positions {{.MAX_TARGET_POSITIONS}} \
              --max-tokens {{.MAX_TOKENS}} \
              --beam {{.BEAM_SIZE}} \
              --sacrebleu \
              --sacrebleu-tokenizer 13a \
              --bpe sentencepiece \
              --sentencepiece-model {{.SPM_MODEL}} \
              --skip-invalid-size-inputs-valid-test > {{.OUT_FILE}}
        silent: false
    vars:
      SEED: 48151623
      MAX_SOURCE_POSITIONS: 1024
      MAX_TARGET_POSITIONS: 1024
      MAX_TOKENS: 16_000
      BEAM_SIZE: 5
      CKPT_FILE:
        sh: echo {{.SAVE_DIR}}/{{.EXPERIMENT}}/ckpts/{{.CKPT}}
      OUT_FILE:
        sh: tmp={{.CKPT_FILE}}; echo "{{.SAVE_DIR}}/{{.EXPERIMENT}}/generates/{{.SUBSET}}/$(basename ${tmp::-3}).out"

